# -*- coding: utf-8 -*-
"""PTTçˆ¬èŸ².ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1huFyf1_rSqC1m3jf5Y7xR57JwDRxtR_2
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime

# è¨­å®šç›®æ¨™æ—¥æœŸï¼ˆä»Šå¤©ï¼‰
target_date = datetime.now().date()

# å»ºç«‹ sessionï¼ˆæ¨¡æ“¬é»é¸ã€Œæˆ‘å·²æ»¿ 18 æ­²ã€ï¼‰
session = requests.Session()
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
})
session.post("https://www.ptt.cc/ask/over18", data={
    "from": "/bbs/Gossiping/index.html",
    "yes": "yes"
})

base_url = "https://www.ptt.cc"
page_url = "/bbs/Gossiping/index.html"
max_pages = 100
page_count = 0
found = 0
stop = False
first_page_done = False

def get_post_datetime(article_url, session):
    try:
        res = session.get(article_url, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        time_tag = soup.select_one("span.article-meta-tag:-soup-contains('æ™‚é–“')")
        if time_tag:
            time_value = time_tag.find_next_sibling("span")
            if time_value:
                return time_value.text.strip()
    except:
        return None
    return None

print(f"ğŸ“Œ æœå°‹ã€ä»Šå¤©ï¼š{target_date}ã€‘çš„æ¨çˆ†æ–‡ç« ï¼ˆç¬¬ä¸€é å®Œæ•´ï¼Œå¾ç¬¬äºŒé èµ·é‡éä»Šæ—¥æ–‡ç« å³åœæ­¢ï¼‰ï¼š\n")

while page_count < max_pages and not stop:
    page_count += 1

    try:
        res = session.get(base_url + page_url, timeout=10)
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ è«‹æ±‚å¤±æ•—ï¼š{e}")
        break

    soup = BeautifulSoup(res.text, "html.parser")
    entries = soup.select("div.r-ent")

    for entry in entries:
        push_raw = entry.select_one("div.nrec").text.strip()
        is_hot = False
        if push_raw == "çˆ†":
            is_hot = True
        else:
            try:
                if int(push_raw) > 99:
                    is_hot = True
            except:
                pass
        if not is_hot:
            continue

        title_tag = entry.select_one("div.title a")
        if not title_tag:
            continue
        title = title_tag.text.strip()
        href = base_url + title_tag["href"]
        author = entry.select_one("div.author").text.strip()

        datetime_str = get_post_datetime(href, session)
        if not datetime_str:
            continue
        try:
            post_dt = datetime.strptime(datetime_str, "%a %b %d %H:%M:%S %Y")
        except ValueError:
            continue

        # ç¬¬äºŒé èµ·åªè¦é‡åˆ°éä»Šæ—¥æ–‡ç« å°±åœ
        if first_page_done and post_dt.date() != target_date:
            stop = True
            print("ğŸ›‘ ç¬¬äºŒé èµ·é‡åˆ°éä»Šæ—¥æ–‡ç« ï¼ŒçµæŸæœå°‹ã€‚")
            break

        # é¡¯ç¤ºç¬¦åˆæ¢ä»¶çš„æ–‡ç« 
        if post_dt.date() == target_date:
            found += 1
            print(f"\n{found}. {title}")
            print(f"   ğŸ•’ ç™¼æ–‡æ™‚é–“ï¼š{datetime_str}")
            print(f"   ğŸ”º æ¨æ–‡æ•¸ï¼š{push_raw}")
            print(f"   ğŸ‘¤ ä½œè€…ï¼š{author}")
            print(f"   ğŸ”— é€£çµï¼š{href}")

    if not first_page_done:
        first_page_done = True

    if stop:
        break

    # ç¿»é 
    prev_link = soup.select_one("div.btn-group-paging a.btn.wide:nth-of-type(2)")
    if not prev_link:
        print("ğŸš« æ‰¾ä¸åˆ°ä¸Šä¸€é ï¼ŒçµæŸã€‚")
        break
    page_url = prev_link["href"]

print(f"\nâœ… çˆ¬èŸ²å®Œæˆï¼šå…±çˆ¬äº† {page_count} é ï¼Œæ‰¾åˆ° {found} ç¯‡ä»Šæ—¥æ¨çˆ†æ–‡ç« ã€‚")