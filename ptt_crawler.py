# -*- coding: utf-8 -*-
"""PTT爬蟲.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1huFyf1_rSqC1m3jf5Y7xR57JwDRxtR_2
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime

# 設定目標日期（今天）
target_date = datetime.now().date()

# 建立 session（模擬點選「我已滿 18 歲」）
session = requests.Session()
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
})
session.post("https://www.ptt.cc/ask/over18", data={
    "from": "/bbs/Gossiping/index.html",
    "yes": "yes"
})

base_url = "https://www.ptt.cc"
page_url = "/bbs/Gossiping/index.html"
max_pages = 100
page_count = 0
found = 0
stop = False
first_page_done = False

def get_post_datetime(article_url, session):
    try:
        res = session.get(article_url, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        time_tag = soup.select_one("span.article-meta-tag:-soup-contains('時間')")
        if time_tag:
            time_value = time_tag.find_next_sibling("span")
            if time_value:
                return time_value.text.strip()
    except:
        return None
    return None

print(f"📌 搜尋【今天：{target_date}】的推爆文章（第一頁完整，從第二頁起遇非今日文章即停止）：\n")

while page_count < max_pages and not stop:
    page_count += 1

    try:
        res = session.get(base_url + page_url, timeout=10)
    except requests.exceptions.RequestException as e:
        print(f"⚠️ 請求失敗：{e}")
        break

    soup = BeautifulSoup(res.text, "html.parser")
    entries = soup.select("div.r-ent")

    for entry in entries:
        push_raw = entry.select_one("div.nrec").text.strip()
        is_hot = False
        if push_raw == "爆":
            is_hot = True
        else:
            try:
                if int(push_raw) > 99:
                    is_hot = True
            except:
                pass
        if not is_hot:
            continue

        title_tag = entry.select_one("div.title a")
        if not title_tag:
            continue
        title = title_tag.text.strip()
        href = base_url + title_tag["href"]
        author = entry.select_one("div.author").text.strip()

        datetime_str = get_post_datetime(href, session)
        if not datetime_str:
            continue
        try:
            post_dt = datetime.strptime(datetime_str, "%a %b %d %H:%M:%S %Y")
        except ValueError:
            continue

        # 第二頁起只要遇到非今日文章就停
        if first_page_done and post_dt.date() != target_date:
            stop = True
            print("🛑 第二頁起遇到非今日文章，結束搜尋。")
            break

        # 顯示符合條件的文章
        if post_dt.date() == target_date:
            found += 1
            print(f"\n{found}. {title}")
            print(f"   🕒 發文時間：{datetime_str}")
            print(f"   🔺 推文數：{push_raw}")
            print(f"   👤 作者：{author}")
            print(f"   🔗 連結：{href}")

    if not first_page_done:
        first_page_done = True

    if stop:
        break

    # 翻頁
    prev_link = soup.select_one("div.btn-group-paging a.btn.wide:nth-of-type(2)")
    if not prev_link:
        print("🚫 找不到上一頁，結束。")
        break
    page_url = prev_link["href"]

print(f"\n✅ 爬蟲完成：共爬了 {page_count} 頁，找到 {found} 篇今日推爆文章。")